# ğŸ›’ RossmannÂ XGBoostÂ Forecasting

> **Accurate sixâ€‘week demand forecasts for Rossmann stores using a fully leakâ€‘free featureâ€‘engineering pipeline and a tuned XGBoost regressor.**

---

## ğŸš¦Â Pipeline Overview

| Stage                  | Script                              | Main Artifact(s)                                                                            |
| ---------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------- |
| 1.â€¯Preâ€‘processing      | `scripts/data_preprocessing.py`     | `data/processed/train_clean.parquet` Â <br> `data/processed/test_clean.parquet`              |
| 2.â€¯Feature engineering | `scripts/feature_engineering.py`    | `data/features/train_fe.parquet` Â <br> `data/features/test_fe.parquet`                      |
| 3.â€¯Leakage checks      | `scripts/check_feature_pipeline.py` | Pass / fail console output                                                                  |
| 4.â€¯Model training      | `scripts/train_model.py`            | `models/xgb_rossmann_<timestamp>.json` Â <br> booster + wrapperÂ \*.pkl, CV & importance CSVs |
| 5.â€¯Holdâ€‘out evaluation | `scripts/evaluate_model.py`         | metricsâ€¯YAML/CSVÂ + residual plot PNG                                                        |
| 6.â€¯Future forecasting  | `scripts/predict_future.py`         | `reports/forecast_<timestamp>.csv` Â <br> optional forecast PNG                              |

---

## ğŸ“‚Â RepositoryÂ Layout

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # original Kaggle CSVs
â”‚   â”œâ”€â”€ processed/        # clean parquet after stageÂ 1
â”‚   â””â”€â”€ features/         # engineered parquet after stageÂ 2
â”‚
â”œâ”€â”€ models/               # saved boosters / wrappers
â”‚
â”œâ”€â”€ reports/              # CV results, feature importance, residuals, forecasts
â”‚   â”œâ”€â”€ *.csv  *.yml  *.png
â”‚   â””â”€â”€ *.log             # timestamped run logs
â”‚
â””â”€â”€ scripts/              # all pipeline code
```

---

## ğŸ› ï¸Â GettingÂ Started

### 1â€¯Â·â€¯Clone & create environment

```bash
git clone https://github.com/yourâ€‘handle/rossmann_xgboost_forecasting.git
cd rossmann_xgboost_forecasting

# Conda (recommended)
conda env create -f environment.yml
conda activate rossmann-xgb

# â€”â€¯orâ€¯â€”
# pip
pip install -r requirements.txt
```

### 2â€¯Â·â€¯Add the Kaggle CSVs

Download **train.csv**, **test.csv**, **store.csv** from the [Kaggle competition page](https://www.kaggle.com/c/rossmann-store-sales/data) and place them in `data/raw/`:

```text
data/raw/
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â””â”€â”€ store.csv
```

### 3â€¯Â·â€¯Run the pipeline endâ€‘toâ€‘end

```bash
# âŠÂ Clean & enrich raw data
python scripts/data_preprocessing.py \
       --raw-dir   data/raw \
       --out-dir   data/processed

# â‹Â Engineer leakageâ€‘free features
python scripts/feature_engineering.py \
       --clean-dir data/processed \
       --out-dir   data/features

# âŒÂ Sanityâ€‘check feature parity & leakage
python scripts/check_feature_pipeline.py  # must finish without assertion errors

# âÂ Train XGBoost with 40â€‘run random search & 5â€‘fold TimeSeriesSplit
python scripts/train_model.py \
       --feature-dir data/features \
       --model-dir   models \
       --reports-dir reports \
       --cv 5 \
       --n-iter 40
```

Sample training output:

```yaml
BestÂ CVÂ RMSE : 751.83
BestÂ params  : {max_depth:Â 10, n_estimators:Â 600, learning_rate:Â 0.05, â€¦}
Artifacts    : models/xgb_rossmann_20250624â€‘134512.{json,pkl}
```

```bash
# âÂ Evaluate on a holdâ€‘out window (last 42Â days of train)
python scripts/evaluate_model.py \
       --feature-dir data/features \
       --model-dir   models \
       --reports-dir reports \
       --horizon 42 \
       --plot            # include to save PNG residual plot

# âÂ Forecast the Kaggle test set (6Â weeks)
python scripts/predict_future.py \
       --feature-dir data/features \
       --model-dir   models \
       --reports-dir reports \
       --plot
```

Generated forecast files:

```text
reports/
â”œâ”€â”€ forecast_YYYYMMDD-hhmmss.csv   # Store,Â Date,Â Sales_pred
â””â”€â”€ forecast_YYYYMMDD-hhmmss.png   # optional visual sanity check
```

Merge `Sales_pred` with the `Id` column from **test.csv** to prepare the Kaggle submission.

---

## ğŸ§©Â FeatureÂ Overview

| Category             | Examples                                                                |
| -------------------- | ----------------------------------------------------------------------- |
| **Lags**             | `Sales_lag1`, `Sales_lag7`, `Sales_lag14`, `Sales_lag28`, `â€¦_lag365`    |
| **Moving windows**   | `Sales_roll7`, `Sales_roll30` + stdâ€‘dev counterparts                    |
| **Promo lookâ€‘ahead** | `Promo_in_1`, `Promo_in_7`, `Promo_in_14`Â â€“ generated leakâ€‘free         |
| **Calendar**         | `Day`, `Month`, `Quarter`, `IsWeekend`, cyclic sin/cos month & ISOÂ week |
| **Holiday distance** | `DaysSincePrevHoliday`, `DaysToNextHoliday` (32767 = none)              |
| **Store priors**     | median sales by `StoreType`, mean by `Assortment`                       |
| **Trend splines**    | cubicÂ Bâ€‘spline basis `Trend_spline_0â€¦4` on a global dayâ€‘index           |

See `reports/feature_importance_<timestamp>.csv` for the ranked list.

---

## ğŸ·ï¸Â KeyÂ Design Decisions

* **Leakâ€‘proofing**Â â€“ every feature uses only past information.  Futureâ€‘promo flags are generated *per split* after concatenating train/test to avoid overlap.
* **TimeSeriesSplit CV**Â â€“ preserves temporal order inside folds.
* **Native categorical** in XGBoostÂ â‰¥Â 1.6Â â€“ faster and lower memory than oneâ€‘hot.
* **Parquet + Zstandard**Â â€“ speedy I/O during iterative modelling.
* **Reproducibility**Â â€“ fixed random seeds, timestamped artifacts, optional Git hash captured in YAML.

---

## ğŸ“ˆÂ Results (holdâ€‘out 42â€¯days)

| Metric    | Value |
| --------- | ----- |
| **RMSE**  | â‰ˆâ€¯450 |
| **MAE**   | â‰ˆâ€¯317 |
| **RMSPE** | 7.9â€¯% |
| **MAPE**  | 5.8â€¯% |

### NextÂ Ideas

* Hierarchical modelling â€“ perâ€‘`StoreType` models blended at inference.
* External regressors â€“ German public holidays, regional weather, CPI.
* Model ensemble â€“ LightGBMÂ +Â CatBoost + Prophet residuals.

---

## ğŸ‘¤ Author

Bhargav Patil

## ğŸ¤ Contributing

1. Fork â†’ create feature branch â†’ commit â†’ open PR.
2. Run `pre-commit run --all-files` (black, isort, flake8) before pushing.
3. Ensure `pytest` passes.

---

## ğŸ“œÂ License

Released under the MITÂ License.  See `LICENSE` for details.
